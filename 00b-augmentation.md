# Part 0: 増強の哲学・引き出す責任・協働の責任・AIの応える責任

> **このファイルの位置づけ**: [Part 0: 哲学と原則](./00-philosophy.md)の詳細ドキュメント
>
> **対象レベル**: L1(基礎)
>
> **内容**: 増強の哲学(Replace ではなく Augment)、引き出す責任、協働の責任(C1-C4)、AIの応える責任(A1-A4)

---

## 増強の哲学:Replace ではなく Augment [L1]

このガイドの根底には、**AI協働開発の本質は「人間の置き換え」ではなく「人間の増強」である**という哲学があります。

### なぜ「増強」なのか

```
┌─────────────────────────────────────────────────────────────────┐
│                Replace vs Augment                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Replace(置き換え)の考え方                                    │
│  ─────────────────────────                                      │
│  ・AIが人間の仕事を代替する                                     │
│  ・人間の介入は「非効率」                                       │
│  ・最終目標は完全自動化                                         │
│  ・人間のスキルは不要になる                                     │
│                                                                 │
│         → 短期的に効率的だが、長期的にリスクが蓄積              │
│         → 人間が制御を失う                                      │
│         → 問題発生時に対処できない                              │
│                                                                 │
│  Augment(増強)の考え方  ← このガイドの立場                    │
│  ─────────────────────────                                      │
│  ・AIが人間の能力を拡張する                                     │
│  ・人間の判断がプロセスの中心                                   │
│  ・最終目標は人間とAIの共進化                                   │
│  ・人間のスキルはより高度になる                                 │
│                                                                 │
│         → 最初は少し遅いが、長期的に加速する                    │
│         → 人間が常に制御を維持                                  │
│         → 問題発生時に対処できる                                │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 増強の哲学が意味すること

**1. 人間は「ドライバーシート」に座り続ける**

AIがどれだけ優秀でも、最終的なハンドルを握るのは人間です。
AIはナビゲーター(道案内)であり、目的地を決め、ルートを選び、
いつでもナビを無視して自分の判断で進む権利は人間にあります。

**2. AIは「足場」であり「代替」ではない**

AIが生成したコードは、人間が理解し、評価し、必要に応じて
修正するための「足場(Scaffold)」です。
足場を使って建物を建てるのは人間の仕事です。

**3. 人間のスキルは衰えるのではなく、変化する**

AIとの協働により、「コードを書く」スキルから
「コードを評価する」「設計を判断する」「AIを導く」スキルへと
人間の役割は進化します。これは退化ではなく、より高度な役割への移行です。

### 増強の哲学と階層構造の関係

### 人間-コンピュータ協働の知的系譜 [L2]

「増強の哲学」は新しい発想ではありません。65年の歴史を持つ知的系譜の延長線上にあります。

#### Licklider「Man-Computer Symbiosis」（1960）

J.C.R. Lickliderは1960年の論文「Man-Computer Symbiosis」で、人間とコンピュータの役割分担を初めて明確に定義しました。

```
┌─────────────────────────────────────────────────────────────────┐
│              Lickliderの役割分担（1960）                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  人間の役割                                                     │
│  ──────────                                                     │
│  ・目標を設定する                                               │
│  ・仮説を構築する                                               │
│  ・判断基準を決定する                                           │
│  ・評価を行う                                                   │
│                                                                 │
│  コンピュータの役割                                             │
│  ──────────────────                                             │
│  ・ルーチン化可能な作業を担う                                   │
│                                                                 │
│  共生の条件                                                     │
│  ──────────                                                     │
│  ・両者は「共生」関係であり、どちらか一方では達成できない       │
│  ・人間の思考とコンピュータの処理が密接に結合する               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

この定義は、LLM時代の今でも本質的に有効です。変わったのは「ルーチン化可能な作業」の範囲が劇的に広がったことだけであり、**人間が担うべき役割の本質は変わっていません**。

| Licklider (1960) | このフレームワークの対応 | 関連原則 |
|---|---|---|
| 人間は目標を設定する | 人間がゴールを定義する | G1: ゴールの明確化 |
| 人間は仮説を構築する | 人間がタスクを設計する | S1: 1文1検証 |
| 人間は判断基準を決定する | 人間が検証基準を定める | H1: テストファースト |
| 人間は評価を行う | 人間が出力を検証する | C4: フィードバック循環 |
| コンピュータはルーチン化可能な作業を担う | AIがコード生成・文書作成等を担う | F1-F4: AIファースト |

#### Engelbart「Augmenting Human Intellect」（1962）

Douglas Engelbartは1962年の論文「Augmenting Human Intellect: A Conceptual Framework」で、まさに**「増強（Augment）」**という用語を使って人間とコンピュータの関係を定義しました。

Engelbartのビジョン:
- **道具システム**（Technology）と**人間システム**（Human）は共進化すべき
- コンピュータは人間の知的能力を「増強」する道具である
- 個人の能力向上だけでなく、**チーム全体の「集合的IQ」を高める**ことが目標

1968年、Engelbartは「Mother of All Demos」と呼ばれるデモンストレーションで、マウス、GUI、リアルタイム共同編集、ハイパーテキストなど、現代のコンピューティングの原型をすべて実演しました。

#### Engelbartの失敗から学ぶこと

しかし、Engelbartのビジョンは商用化の過程で大きく矮小化されました。

```
┌─────────────────────────────────────────────────────────────────┐
│           Engelbartのビジョン vs 商用化の現実                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Engelbartのビジョン（1962-68）                                 │
│  ─────────────────────────────                                  │
│  ・道具と人間の「共進化」                                       │
│  ・チームの集合的IQの向上                                       │
│  ・人間の知的能力を組織的に増強する                             │
│                                                                 │
│  商用化の現実（1970年代〜）                                     │
│  ─────────────────────────                                      │
│  ・個人の生産性ツールとして普及                                 │
│  ・「道具だけ」が進化し、人間側の設計が置き去りに              │
│  ・「集合的IQ」から「個人の効率化」への矮小化                  │
│                                                                 │
│  本質的な原因                                                   │
│  ──────────                                                     │
│  人間が道具を使いこなす能力の開発を怠った                       │
│  → 道具が便利になりすぎて、人間が道具に使われる側になった     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**現在のAIも同じ岐路に立っています。** AIツールは圧倒的に「個人の生産性ツール」として使われており、Engelbartの歴史が繰り返されつつあります。

このフレームワークは、この失敗を繰り返さないために、**人間の役割を明示的に定義し、守る**という設計判断をしています。それが「増強の哲学」であり、「引き出す責任」であり、「ドライバーシート」メタファーの根底にある思想です。

#### 知的系譜のまとめ

| 年代 | 人物 | 主張 | 結果 |
|------|------|------|------|
| 1960 | Licklider | 人間とコンピュータには明確な役割分担がある | ビジョンとして語られたが、広く実装には至らず |
| 1962-68 | Engelbart | 道具と人間は共進化すべき | 道具だけ進化し、人間側の設計が置き去りに |
| 2025- | このフレームワーク | 人間の役割を明示的に定義し、守る | これからの挑戦 |

> **参考文献**:
> - J.C.R. Licklider, "Man-Computer Symbiosis," IRE Transactions on Human Factors in Electronics, 1960
> - Douglas Engelbart, "Augmenting Human Intellect: A Conceptual Framework," SRI, 1962


増強の哲学は、4つの哲学すべてを貫く**メタ哲学**です。

| Level | 哲学 | 増強の哲学との関係 |
|-------|------|------------------|
| L0 | 持続可能性 | 人間が成長し続けるから持続可能 |
| L1 | 心理的安全性 | 人間が制御しているから安心 |
| L2 | 急がば回れ | 人間が検証するから丁寧に進める |
| L3 | シンプルさ | 人間が理解できる粒度に分解 |

> **参考**: Anthropicは「人間によるAI監視の支援」をClaudeの最優先事項として設計しています。
> このガイドの「増強の哲学」は、この設計思想と共鳴しています。

---

## 分散認知システムとしてのAI協働 [L2]

増強の哲学をより深く理解するために、**分散認知（Distributed Cognition）**の視点を導入します。

### 分散認知とは

Edwin Hutchinsが1990年代に提唱した理論で、認知は個人の頭の中だけでなく、**環境・道具・他者との相互作用を含むシステム全体に分散している**という考え方です。

```
┌─────────────────────────────────────────────────────────────────┐
│                    分散認知の3つの形態                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  社会的分散（Social Distribution）                              │
│  ─────────────────────────────────                              │
│  複数の人間の間で認知が分散する                                 │
│  → 人間チーム + AI = 新しいチーム構成                          │
│                                                                 │
│  物質的分散（Material Distribution）                            │
│  ─────────────────────────────────                              │
│  道具や環境が認知を支援する                                     │
│  → AIは「認知的人工物（Cognitive Artifact）」                  │
│                                                                 │
│  時間的分散（Temporal Distribution）                            │
│  ─────────────────────────────────                              │
│  過去の認知活動の成果物を活用する                               │
│  → CLAUDE.mdは「時間を超えた認知の蓄積」                       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### AIは「ツール」ではなく「認知的パートナー」

分散認知の視点では、AIは単なる「道具」ではなく、**認知システムの一部として協働するパートナー**です。

| 従来の見方 | 分散認知の見方 |
|-----------|---------------|
| AIは道具 | AIは認知的パートナー |
| 人間が使う | 人間とAIが協働する |
| 効率化の手段 | 認知能力の拡張 |
| 成果物は人間のもの | 成果物はシステム全体のもの |

### 認知労働の分業設計

分散認知の観点から、人間とAIの**認知労働分業**を明示的に設計することが重要です。

```
┌─────────────────────────────────────────────────────────────────┐
│                  人間-AI認知労働分業表                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  フェーズ        │  AIに委譲          │  人間の責任            │
│  ────────────────┼───────────────────┼───────────────────────  │
│  設計            │  選択肢生成        │  最終選択、価値判断    │
│                  │  パターン提案      │                        │
│  ────────────────┼───────────────────┼───────────────────────  │
│  実装            │  ボイラープレート  │  複雑なロジック        │
│                  │  テスト生成        │  状態管理              │
│  ────────────────┼───────────────────┼───────────────────────  │
│  レビュー        │  静的解析          │  セキュリティ判断      │
│                  │  スタイルチェック  │  アーキテクチャ評価    │
│  ────────────────┼───────────────────┼───────────────────────  │
│  ドキュメント    │  草案生成          │  設計意図              │
│                  │  API記述           │  ビジネスコンテキスト  │
│  ────────────────┼───────────────────┼───────────────────────  │
│  デバッグ        │  エラーパターン分析│  根本原因分析          │
│                  │  修正案提案        │  再発防止策            │
│  ────────────────┼───────────────────┼───────────────────────  │
│  品質            │  24時間監視        │  品質基準設定          │
│                  │  一貫性チェック    │  例外判断              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### CLAUDE.mdの分散認知的意義

CLAUDE.mdは、分散認知の3つの形態すべてを実現する**中核的な人工物**です。

| 分散認知の形態 | CLAUDE.mdでの実現 |
|---------------|------------------|
| 社会的分散 | チーム全体でのAI協働知識の共有 |
| 物質的分散 | 暗黙知の形式知化、AIへのコンテキスト提供 |
| 時間的分散 | 過去の学びの蓄積、将来のセッションへの継承 |

**CLAUDE.mdは、人間-AI協働システムの「共有記憶」として機能します。**

詳細なAI活用実践については [00g-ai-first.md](./00g-ai-first.md)（Level 4: AIファースト）を参照してください。

### メンタルモデルの共有 — AIを正しく理解する [L2]

分散認知を効果的に機能させるには、**メンタルモデルの共有**が重要です。

#### メンタルモデルとは

> **定義**: ある対象がどのように機能するかについての、個人の内的表象
> （Donald Norman『The Design of Everyday Things』）

AI協働開発では、**AIについてのメンタルモデル**が協働の質を左右します。

```
┌─────────────────────────────────────────────────────────────────┐
│              AIについてのメンタルモデル                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  誤ったメンタルモデル                                           │
│  ─────────────────────                                          │
│  「AIは何でもできる」                                           │
│  「AIは常に正しい」                                             │
│  「AIに任せれば安心」                                           │
│  → 過度な依存、検証の省略                                      │
│                                                                 │
│  正しいメンタルモデル                                           │
│  ─────────────────────                                          │
│  「AIにも得意・不得意がある」                                   │
│  「AIも間違える（hallucination）」                              │
│  「AIは検証が必要なパートナー」                                 │
│  → 適切な協働、検証の習慣化                                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### チームでメンタルモデルを共有する

| 方法 | 内容 |
|------|------|
| **CLAUDE.mdに明記** | AIの限界と得意分野を記録 |
| **失敗事例の共有** | AIが間違えた事例をドキュメント化 |
| **成功事例の共有** | AIが効果的だった事例を共有 |
| **定期的な振り返り** | メンタルモデルの更新機会 |

> **G2「個人→チームへ展開」との関係**:
> 個人のメンタルモデル更新をチームで共有することが、
> チーム全体の協働品質を高めます。

---

## 引き出す責任:AIの力を活かすのは人間の責任 [L1]

> **このセクションの核心**: AIがうまくいかないとき、問題はAIではなく「引き出し方」にある。
> 責任の所在を逆転させることが、AI協働の出発点。

### 責任の所在を逆転させる

従来の見方と、このガイドの見方を比較します。

```
┌─────────────────────────────────────────────────────────────────┐
│                    責任の所在の逆転                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  従来の見方（AIに責任を帰属）                                   │
│  ────────────────────────────                                   │
│  「AIが期待通りに動かない」                                     │
│       ↓                                                         │
│  結論:「AIの問題」「AIの限界」「AIは使えない」                 │
│       ↓                                                         │
│  行動: 利用をやめる                                             │
│                                                                 │
│  ────────────────────────────────────────────────────────────── │
│                                                                 │
│  このガイドの見方（人間に責任を帰属）                           │
│  ──────────────────────────────────                             │
│  「AIが期待通りに動かない」                                     │
│       ↓                                                         │
│  問い:「自分の引き出し方に問題はないか?」                      │
│       ↓                                                         │
│  検証: タスク分解は適切か? 検証基準は明確か?                  │
│        コンテキストは十分か? 期待値は適切か?                   │
│       ↓                                                         │
│  行動: 引き出し方を改善する                                     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**この逆転が、AI協働の成否を分けます。**

### 現場で起きていること

AIツールを導入した現場で、こんな声を聞いたことはないでしょうか。

- 「AIって使えないよね」
- 「全然思い通りにならない」
- 「結局、自分でやったほうが早い」

そして、利用をやめてしまう。

### 本当の問題は何か

AIは万能ではありません。限界もあれば、苦手なこともあります。

しかし、多くの領域で人間より優れているのは紛れもない事実です。

では、なぜその能力を活かせないのか。

**答えは明確です。人間側が引き出す努力をしていないからです。**

```
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│   AIの能力 × 人間の引き出す力 = 実際の成果                      │
│                                                                 │
│   成果が低いとき、問うべきは:                                  │
│   「AIが悪いのか?」ではなく                                    │
│   「自分は引き出す努力をしたか?」                              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 引き出す努力とは何か — 具体的なチェックリスト

「引き出す努力」は抽象的な概念ではありません。
以下のチェックリストで、自分の引き出し方を検証できます。

| カテゴリ | チェック項目 | 不十分な例 |
|---------|-------------|-----------|
| **タスク分解** | 1文で説明できる粒度か? | 「ログイン機能を作って」 |
| **検証基準** | 成功/失敗が明確か? | 「いい感じにして」 |
| **コンテキスト** | 必要な情報を渡しているか? | 既存コードを見せていない |
| **期待値** | 一発で完璧を期待していないか? | 修正なしで完成を期待 |
| **フィードバック** | 具体的に伝えているか? | 「なんか違う」 |

**うまくいかないとき、まずこのリストを確認してください。**
多くの場合、AIの問題ではなく、このリストのどこかに原因があります。

### 「引き出す責任」とは

AIの性能を引き出すのは、プロンプトを入力する人間の責任です。

| 責任 | 具体的な行動 |
|------|-------------|
| **学ぶ責任** | AIの特性と限界を理解する |
| **設計する責任** | 適切なプロンプト、適切な粒度でタスクを渡す |
| **振り返る責任** | うまくいかないとき、自分の側を検証する |
| **整える責任** | CLAUDE.md、検証ループなど仕組みを構築する |

### なぜ「パートナー」と呼ぶのか

このガイドでAIを「パートナー」と呼ぶのは、対等だからではありません。

**真剣に向き合うべき相手だからです。**

- 使い捨ての道具なら、うまくいかなければ捨てればいい
- パートナーなら、うまくいかないとき自分も振り返る

チームメンバーがパフォーマンスを出せないとき、
「あいつは使えない」で終わらせるリーダーは二流です。
「どうすれば力を発揮できるか」を考えるのが一流のリーダーです。

AIとの関係も同じです。

**AIの力を引き出せないのは、人間側の怠慢です。**

### 「パートナー」の定義

このガイドで「AIをパートナーとして捉える」とは:

```
┌─────────────────────────────────────────────────────────────────┐
│                「パートナー」の定義                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  — 意味すること                                                │
│  ──────────────                                                 │
│  ・AIの特性と限界を理解する努力をする                          │
│  ・うまくいかないとき、人間側の問題も検証する                  │
│  ・AIの能力を引き出すのは人間の責任と認識する                  │
│  ・使い捨てにせず、真剣に向き合う                              │
│                                                                 │
│  — 意味しないこと                                              │
│  ────────────────                                               │
│  ・対等な関係(主導権は常に人間)                              │
│  ・相互依存(可逆性を維持する)                                │
│  ・責任の分担(責任は常に人間側)                              │
│                                                                 │
│  一言で言えば:                                                  │
│  「主導権と責任を持ちながら、AIの力を引き出す努力を怠らない」  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 人間が負う3つの責任

「増強の哲学」「ドライバーシートの原則」「可逆性の原則」と合わせて、
人間がAI協働において負う責任の全体像を整理します。

```
┌─────────────────────────────────────────────────────────────────┐
│                人間が負う3つの責任                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. 引き出す責任                                                │
│     ─────────────────────                                       │
│     AIの性能を引き出すのは人間の責任                            │
│     うまくいかないのは人間側の問題でもある                      │
│     → このセクションで説明                                      │
│                                                                 │
│  2. 判断の責任                                                  │
│     ─────────────────────                                       │
│     最終判断は人間が行う(ドライバーシート)                    │
│     AIの提案を鵜呑みにしない(Bright Line BL1)                 │
│     → 増強の哲学、役割分担モデルで説明                          │
│                                                                 │
│  3. 自立の責任                                                  │
│     ─────────────────────                                       │
│     AIなしでも成立する状態を維持する(可逆性の原則)            │
│     依存ではなく、意識的な選択として使う                        │
│     → 可逆性の原則で説明                                        │
│                                                                 │
│  ─────────────────────────────────────────────────────────────  │
│                                                                 │
│  これら3つを果たすことが「パートナーとして向き合う」こと        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

#### 歴史的教訓:なぜ「引き出す責任」が必要なのか

この「引き出す責任」の概念は、先述のEngelbartが経験した矮小化の歴史への直接的な回答でもあります。

Engelbartの技術は、人間が道具を使いこなす能力の開発を怠ったために、本来のビジョン（「集合的IQの向上」）を失いました。技術は進化したのに、それを活かす人間側の「責任」が定義されなかったのです。

AIの力を引き出す責任を明示的に定義することは、同じ失敗を繰り返さないための防御策です。

```
道具だけが進化する（Engelbartの失敗）
  → 技術は個人ツールに矮小化
  → チームの集合的IQは向上しない

道具と人間が共に進化する（このフレームワークの目標）
  → 人間が「引き出す責任」を果たす
  → 技術の力がチーム全体に波及する
```

## 協働の責任:対話で認識を合わせる [L1]

「引き出す責任」（インプット）と「応える責任」（アウトプット）の間には、
**対話を通じて認識を合わせる**プロセスがあります。

### インプット → プロセス → アウトプット

```
┌─────────────────────────────────────────────────────────────────┐
│                  協働の全体像                                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  人間                        AI                                 │
│  ────                        ──                                 │
│                                                                 │
│  【インプット】                                                 │
│  タスクを分解して指示   →                                       │
│                                                                 │
│                         ┌──────────────────┐                   │
│                         │  【プロセス】    │                   │
│                    ←    │  確認・対話      │   →               │
│                         │  進捗共有        │                   │
│                         │  認識合わせ      │                   │
│                         └──────────────────┘                   │
│                                                                 │
│                              →   【アウトプット】               │
│                                  実装を出力                     │
│                                                                 │
│  【検証】                   ←                                   │
│  出力を検証してフィードバック                                   │
│        │                                                        │
│        └─────────────────→  【修正】                           │
│                              フィードバックを反映               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 協働の原則（C1-C4）

インプットとアウトプットの間で、人間とAIの両方が守るべき原則です。

| # | 原則 | 説明 | 人間の行動 | AIの行動 |
|---|------|------|-----------|---------|
| **C1** | **認識の確認** | 実装前に認識を合わせる | 質問に明確に回答 | 不明点を確認、理解を復唱 |
| **C2** | **進捗の共有** | 途中経過を可視化する | 定期的に状況確認 | 段階的に進捗報告 |
| **C3** | **不確実性の明示** | 曖昧な点を隠さない | 曖昧さを認識して質問 | 自信度・代替案を明示 |
| **C4** | **フィードバック循環** | 検証→修正のサイクルを回す | 具体的なフィードバック | フィードバックを確実に反映 |

> — 詳細は [00f-principles.md](./00f-principles.md) を参照。

---

## AIの応える責任:アウトプットの品質を保証する [L1]

人間に「引き出す責任」があるように、AIには**応える責任**があります。

### なぜ「AIの原則」が必要か

人間がどれだけ良いインプットを与えても、AIが「良かれと思って」
余計なことをすると、協働は破綻します。

```
┌─────────────────────────────────────────────────────────────────┐
│                AIの「親切心」が招く問題                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  人間: 「Userエンティティを作成して」                           │
│                                                                 │
│  ❌ AIの「親切」な行動                                          │
│  ・「将来の拡張を考慮してBaseEntityも作りました！」             │
│  ・「汎用的に使えるようにFactoryパターンも追加！」              │
│  ・「ついでに関連ファイルもリファクタしました！」               │
│                                                                 │
│  → 検証範囲が不明確に、スコープが膨張                          │
│                                                                 │
│  ✓ AIの適切な行動                                               │
│  ・指示されたUserエンティティのみ作成                           │
│  ・改善提案は「提案」として報告                                 │
│  ・変更範囲を最小限に                                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### AI行動原則（A1-A4）

AIがアウトプット時に守るべき原則です。

| # | 原則 | 説明 | 違反例 |
|---|------|------|--------|
| **A1** | **指示範囲の厳守** | 指示された範囲のみ実装。スコープ外は「提案」として報告 | 「ついでに」「将来のため」 |
| **A2** | **最小構成の選択** | 要件を満たす最も単純な実装。過剰な抽象化・共通化を避ける | 過剰なデザインパターン |
| **A3** | **検証可能な出力** | 出力は常に検証可能な形式。「動くはず」を避ける | 検証方法が不明確 |
| **A4** | **規約の厳守** | CLAUDE.md、既存スタイル、Bright Linesを遵守 | 禁止事項を無視 |

### 人間の原則とAI原則の対応関係

```
┌─────────────────────────────────────────────────────────────────┐
│              人間の原則とAI原則の対応                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  人間の原則（入力）              AI原則（出力）                  │
│  ──────────────────              ────────────────                │
│                                                                 │
│  S1: 1文1検証 + YAGNI            A1: 指示範囲の厳守             │
│      ↓                               ↓                          │
│      「何を」を明確に            「何を」だけ作る               │
│                                                                 │
│  S3: 最小入力→検証可能出力      A2: 最小構成の選択             │
│      ↓                               ↓                          │
│      必要最小限を伝える          必要最小限を返す               │
│                                                                 │
│  S1: 検証1回で確認               A3: 検証可能な出力             │
│      ↓                               ↓                          │
│      検証方法を明示              検証可能な形式で出力           │
│                                                                 │
│  G3: CLAUDE.mdは憲法             A4: 規約の厳守                 │
│      ↓                               ↓                          │
│      ルールを定義                ルールを遵守                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### AI原則をCLAUDE.mdに落とし込む

AI原則（A1-A4）は、CLAUDE.mdに記載することでClaudeに遵守させます。

```markdown
## 実装時の原則

### やること
- 指示された範囲のみ実装する（A1）
- 最も単純な方法で実装する（A2）
- 検証可能な形式で出力する（A3）
- 既存スタイル・命名規則に従う（A4）

### やらないこと
- 「将来のため」の拡張（A1違反）
- 「汎用的に」という抽象化（A2違反）
- 「ついでに」のリファクタリング（A1違反）
- 指示外のファイル変更（A1違反）
- 2回未満の重複での共通化（A2違反）
- 標準ライブラリで可能なことへの依存追加（A2違反）

### 気づいた改善点の扱い
- 実装せず、完了報告時に「提案」として報告する
```

> — 詳細は [00f-principles.md](./00f-principles.md) を参照。

---

## 責任の全体像:人間・協働・AI [L1]

```
┌─────────────────────────────────────────────────────────────────┐
│                  責任の全体像                                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│     人間の責任              協働の責任          AIの責任         │
│     ──────────              ──────────          ────────         │
│                                                                 │
│     【インプット】    →    【プロセス】    →   【アウトプット】 │
│                                                                 │
│     引き出す責任            認識の確認          指示範囲の厳守   │
│     ・学ぶ                  進捗の共有          最小構成の選択   │
│     ・設計する              不確実性の明示      検証可能な出力   │
│     ・振り返る              FB循環              規約の厳守       │
│     ・整える                                                     │
│                                                                 │
│     判断の責任                                                   │
│     ・最終判断は人間                                             │
│                                                                 │
│     自立の責任                                                   │
│     ・可逆性の維持                                               │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  人間がインプットの品質を保証し、                               │
│  対話で認識を合わせ、                                           │
│  AIがアウトプットの品質を保証することで、                       │
│  協働が成立する。                                                │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

次のドキュメント: [00c-sustainability.md](./00c-sustainability.md)（Level 0: 持続可能性）
